{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "DDaTTHt2a7XO"
   },
   "outputs": [],
   "source": [
    "from json import load\n",
    "import torch\n",
    "from torchvision.datasets import ImageFolder\n",
    "from torchvision.transforms import ToTensor,Compose, Resize, Normalize\n",
    "from torch.utils.data.dataloader import DataLoader\n",
    "from torch.utils.data import random_split\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from PIL import Image\n",
    "import os \n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from pprint import pprint as pp\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Analysis\n",
    "Modelos para extração de carcateristicas com DINO- Versões 1 e 2 e suas variações\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /home/pdi_4/.cache/torch/hub/facebookresearch_dino_main\n",
      "Using cache found in /home/pdi_4/.cache/torch/hub/facebookresearch_dino_main\n",
      "Using cache found in /home/pdi_4/.cache/torch/hub/facebookresearch_dino_main\n",
      "Using cache found in /home/pdi_4/.cache/torch/hub/facebookresearch_dino_main\n",
      "Using cache found in /home/pdi_4/.cache/torch/hub/facebookresearch_dinov2_main\n",
      "/home/pdi_4/.cache/torch/hub/facebookresearch_dinov2_main/dinov2/layers/swiglu_ffn.py:51: UserWarning: xFormers is not available (SwiGLU)\n",
      "  warnings.warn(\"xFormers is not available (SwiGLU)\")\n",
      "/home/pdi_4/.cache/torch/hub/facebookresearch_dinov2_main/dinov2/layers/attention.py:33: UserWarning: xFormers is not available (Attention)\n",
      "  warnings.warn(\"xFormers is not available (Attention)\")\n",
      "/home/pdi_4/.cache/torch/hub/facebookresearch_dinov2_main/dinov2/layers/block.py:40: UserWarning: xFormers is not available (Block)\n",
      "  warnings.warn(\"xFormers is not available (Block)\")\n",
      "Using cache found in /home/pdi_4/.cache/torch/hub/facebookresearch_dinov2_main\n",
      "Using cache found in /home/pdi_4/.cache/torch/hub/facebookresearch_dinov2_main\n",
      "Using cache found in /home/pdi_4/.cache/torch/hub/facebookresearch_dinov2_main\n",
      "Using cache found in /home/pdi_4/.cache/torch/hub/facebookresearch_dinov2_main\n",
      "Using cache found in /home/pdi_4/.cache/torch/hub/facebookresearch_dinov2_main\n",
      "Using cache found in /home/pdi_4/.cache/torch/hub/facebookresearch_dinov2_main\n",
      "Using cache found in /home/pdi_4/.cache/torch/hub/facebookresearch_dinov2_main\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#Dino V1\n",
    "vits8 = torch.hub.load('facebookresearch/dino:main', 'dino_vits8') # funcionando\n",
    "vitb8 = torch.hub.load('facebookresearch/dino:main', 'dino_vitb8')\n",
    "vits16 = torch.hub.load('facebookresearch/dino:main', 'dino_vits16')\n",
    "vitb16 = torch.hub.load('facebookresearch/dino:main', 'dino_vitb16')\n",
    "\n",
    "# DINOv2\n",
    "dinov2_vits14 = torch.hub.load('facebookresearch/dinov2', 'dinov2_vits14')\n",
    "dinov2_vitb14 = torch.hub.load('facebookresearch/dinov2', 'dinov2_vitb14')\n",
    "dinov2_vitl14 = torch.hub.load('facebookresearch/dinov2', 'dinov2_vitl14')\n",
    "dinov2_vitg14 = torch.hub.load('facebookresearch/dinov2', 'dinov2_vitg14')\n",
    "\n",
    "# DINOv2 with registers\n",
    "dinov2_vits14_reg = torch.hub.load('facebookresearch/dinov2', 'dinov2_vits14_reg')\n",
    "dinov2_vitb14_reg = torch.hub.load('facebookresearch/dinov2', 'dinov2_vitb14_reg')\n",
    "dinov2_vitl14_reg = torch.hub.load('facebookresearch/dinov2', 'dinov2_vitl14_reg')\n",
    "dinov2_vitg14_reg = torch.hub.load('facebookresearch/dinov2', 'dinov2_vitg14_reg')\n",
    "\n",
    "# xcit_small_12_p16 = torch.hub.load('facebookresearch/dino:main', 'dino_xcit_small_12_p16')\n",
    "# xcit_small_12_p8 = torch.hub.load('facebookresearch/dino:main', 'dino_xcit_small_12_p8')\n",
    "# xcit_medium_24_p16 = torch.hub.load('facebookresearch/dino:main', 'dino_xcit_medium_24_p16')\n",
    "# xcit_medium_24_p8 = torch.hub.load('facebookresearch/dino:main', 'dino_xcit_medium_24_p8')\n",
    "# resnet50 = torch.hub.load('facebookresearch/dino:main', 'dino_resnet50')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DINO Perceptual Losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class PerceptualLoss(nn.Module):\n",
    "    def __init__(self, model, layers=None, normalize_inputs=False):\n",
    "        \"\"\"\n",
    "        Perceptual loss using DINO or DINOv2 models.\n",
    "\n",
    "        Args:\n",
    "            model (torch.nn.Module): The DINO model to extract features.\n",
    "            layers (list of str): Names of layers to use for loss computation.\n",
    "                                  Default is None, which uses all layers.\n",
    "            normalize_inputs (bool): Whether to normalize inputs to [0, 1].\n",
    "        \"\"\"\n",
    "        super(PerceptualLoss, self).__init__()\n",
    "        self.model = model.eval()  # Set model to evaluation mode\n",
    "        self.layers = layers\n",
    "        self.normalize_inputs = normalize_inputs\n",
    "\n",
    "        # Disable gradient computation for the model\n",
    "        for param in self.model.parameters():\n",
    "            param.requires_grad = False\n",
    "\n",
    "    def _ensure_tensor(self, feat):\n",
    "            if isinstance(feat, tuple):\n",
    "                feat = feat[0]\n",
    "            return feat\n",
    "    \n",
    "    def extract_features(self, x):\n",
    "        \"\"\"\n",
    "        Extract features from the model.\n",
    "\n",
    "        Args:\n",
    "            x (torch.Tensor): Input image tensor (B, C, H, W).\n",
    "\n",
    "        Returns:\n",
    "            list of torch.Tensor: Extracted features.\n",
    "        \"\"\"\n",
    "        features = []\n",
    "        hooks = []\n",
    "\n",
    "        # Hook to extract features from specified layers\n",
    "        def hook_fn(module, input, output):\n",
    "            features.append(output)\n",
    "\n",
    "        # Register hooks on specified layers or use all layers\n",
    "        if self.layers is None:\n",
    "            for name, module in self.model.named_modules():\n",
    "                hooks.append(module.register_forward_hook(hook_fn))\n",
    "        else:\n",
    "            for name, module in self.model.named_modules():\n",
    "                if name in self.layers:\n",
    "                    hooks.append(module.register_forward_hook(hook_fn))\n",
    "\n",
    "        # Forward pass to get features\n",
    "        self.model(x)\n",
    "\n",
    "        # Remove hooks\n",
    "        for hook in hooks:\n",
    "            hook.remove()\n",
    "\n",
    "        return features\n",
    "\n",
    "\n",
    "    def forward(self, input, target):\n",
    "        \n",
    "        \"\"\"\n",
    "        Compute perceptual loss between input and target images.\n",
    "\n",
    "        Args:\n",
    "            input (torch.Tensor): Input image tensor (B, C, H, W).\n",
    "            target (torch.Tensor): Target image tensor (B, C, H, W).\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: Perceptual loss value.\n",
    "        # \"\"\"\n",
    "        if self.normalize_inputs:\n",
    "            input = (input - input.min()) / (input.max() - input.min())\n",
    "            target = (target - target.min()) / (target.max() - target.min())\n",
    "\n",
    "        # Extract features\n",
    "        input_features = self.extract_features(input)\n",
    "        target_features = self.extract_features(target)\n",
    "\n",
    "        # Compute loss\n",
    "        loss = 0\n",
    "        for inp_feat, tgt_feat in zip(input_features, target_features):\n",
    "            \n",
    "            inp_feat = self._ensure_tensor(inp_feat)\n",
    "            tgt_feat = self._ensure_tensor(tgt_feat)\n",
    "\n",
    "            loss += nn.functional.smooth_l1_loss(inp_feat, tgt_feat).to('cpu')\n",
    "        return loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "# Função para medir o tempo de execução\n",
    "def measure_execution_time(model, input, target):\n",
    "    start_time = time.time()\n",
    "    loss = model(input, target)\n",
    "    end_time = time.time()\n",
    "    print(f\"Tempo de execução: {end_time - start_time} segundos\")\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Funcionamento\n",
    "\n",
    "Esse codigo funciona para extrair features e calacular uma loss function baseada nessa comparação de features com uma função de perda. Aplicados assim nas arquiteturas pre-treinadas de versoes do DINO. O objetivo é usar versões diferentes do DINO como funções perceptuais. \n",
    "* Funcionando\n",
    "    * DINO V1 loss functions completmante funcionais. (DONE)\n",
    "    * DINO V2\n",
    "    * DINO V2 with registers\n",
    "\n",
    "    \n",
    "**Observações**\n",
    "\n",
    "\n",
    "Esta sendo usada a smooth L1 por sua melhor aplicabilidade se comparada a MSE e a L! individualmente.\n",
    "\n",
    "    \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Dino V1\n",
    "# vits8 = torch.hub.load('facebookresearch/dino:main', 'dino_vits8') # funcionando\n",
    "# vitb8 = torch.hub.load('facebookresearch/dino:main', 'dino_vitb8')\n",
    "# vits16 = torch.hub.load('facebookresearch/dino:main', 'dino_vits16')\n",
    "# vitb16 = torch.hub.load('facebookresearch/dino:main', 'dino_vitb16')\n",
    "\n",
    "# # DINOv2\n",
    "# dinov2_vits14 = torch.hub.load('facebookresearch/dinov2', 'dinov2_vits14')\n",
    "# dinov2_vitb14 = torch.hub.load('facebookresearch/dinov2', 'dinov2_vitb14')\n",
    "# dinov2_vitl14 = torch.hub.load('facebookresearch/dinov2', 'dinov2_vitl14')\n",
    "# dinov2_vitg14 = torch.hub.load('facebookresearch/dinov2', 'dinov2_vitg14')\n",
    "\n",
    "# # DINOv2 with registers\n",
    "# dinov2_vits14_reg = torch.hub.load('facebookresearch/dinov2', 'dinov2_vits14_reg')\n",
    "# dinov2_vitb14_reg = torch.hub.load('facebookresearch/dinov2', 'dinov2_vitb14_reg')\n",
    "# dinov2_vitl14_reg = torch.hub.load('facebookresearch/dinov2', 'dinov2_vitl14_reg')\n",
    "# dinov2_vitg14_reg = torch.hub.load('facebookresearch/dinov2', 'dinov2_vitg14_reg')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DINO V1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /home/pdi_4/.cache/torch/hub/facebookresearch_dino_main\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input image: torch.Size([1, 3, 224, 224]) tensor(7.4506e-06) tensor(1.0000)\n",
      "Target image: torch.Size([1, 3, 224, 224]) tensor(2.3842e-06) tensor(1.0000)\n",
      "Perda perceptual: 37.65395736694336\n",
      "Tempo de execução: 0.41363000869750977 segundos\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor(37.6540)"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Exemplos de uso\n",
    "# Escolha o modelo desejado\n",
    "model = torch.hub.load('facebookresearch/dino:main', 'dino_vits8')\n",
    "\n",
    "# Instancie a função de perda perceptual\n",
    "perceptual_loss = PerceptualLoss(model=model,normalize_inputs=False)\n",
    "\n",
    "# Defina imagens de exemplo\n",
    "input_image = torch.rand(1, 3, 224, 224)  # Batch de 1 imagem (C, H, W)\n",
    "target_image = torch.rand(1, 3, 224, 224)\n",
    "\n",
    "print(\"Input image:\", input_image.shape, input_image.min(), input_image.max())\n",
    "print(\"Target image:\", target_image.shape, target_image.min(), target_image.max())\n",
    "# Compute a perda perceptual\n",
    "loss_value = perceptual_loss(input_image, target_image)\n",
    "print(\"Perda perceptual:\", loss_value.item())\n",
    "\n",
    "measure_execution_time(perceptual_loss, input_image, target_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /home/pdi_4/.cache/torch/hub/facebookresearch_dino_main\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input image: torch.Size([1, 3, 224, 224]) tensor(3.0994e-06) tensor(1.0000)\n",
      "Target image: torch.Size([1, 3, 224, 224]) tensor(4.1723e-06) tensor(1.0000)\n",
      "Perda perceptual: 20.65361213684082\n",
      "Tempo de execução: 1.1973350048065186 segundos\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor(20.6536)"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# Exemplos de uso\n",
    "# Escolha o modelo desejado\n",
    "model = torch.hub.load('facebookresearch/dino:main', 'dino_vitb8')\n",
    "\n",
    "# Instancie a função de perda perceptual\n",
    "perceptual_loss = PerceptualLoss(model=model,normalize_inputs=False)\n",
    "\n",
    "# Defina imagens de exemplo\n",
    "input_image = torch.rand(1, 3, 224, 224)  # Batch de 1 imagem (C, H, W)\n",
    "target_image = torch.rand(1, 3, 224, 224)\n",
    "\n",
    "print(\"Input image:\", input_image.shape, input_image.min(), input_image.max())\n",
    "print(\"Target image:\", target_image.shape, target_image.min(), target_image.max())\n",
    "# Compute a perda perceptual\n",
    "loss_value = perceptual_loss(input_image, target_image)\n",
    "print(\"Perda perceptual:\", loss_value.item())\n",
    "\n",
    "measure_execution_time(perceptual_loss, input_image, target_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /home/pdi_4/.cache/torch/hub/facebookresearch_dino_main\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input image: torch.Size([1, 3, 224, 224]) tensor(1.1921e-05) tensor(1.0000)\n",
      "Target image: torch.Size([1, 3, 224, 224]) tensor(1.0192e-05) tensor(1.0000)\n",
      "Perda perceptual: 37.05283737182617\n",
      "Tempo de execução: 0.10273957252502441 segundos\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor(37.0528)"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# Exemplos de uso\n",
    "# Escolha o modelo desejado\n",
    "model = torch.hub.load('facebookresearch/dino:main', 'dino_vits16')\n",
    "\n",
    "# Instancie a função de perda perceptual\n",
    "perceptual_loss = PerceptualLoss(model=model,normalize_inputs=False)\n",
    "\n",
    "# Defina imagens de exemplo\n",
    "input_image = torch.rand(1, 3, 224, 224)  # Batch de 1 imagem (C, H, W)\n",
    "target_image = torch.rand(1, 3, 224, 224)\n",
    "\n",
    "print(\"Input image:\", input_image.shape, input_image.min(), input_image.max())\n",
    "print(\"Target image:\", target_image.shape, target_image.min(), target_image.max())\n",
    "# Compute a perda perceptual\n",
    "loss_value = perceptual_loss(input_image, target_image)\n",
    "print(\"Perda perceptual:\", loss_value.item())\n",
    "\n",
    "measure_execution_time(perceptual_loss, input_image, target_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /home/pdi_4/.cache/torch/hub/facebookresearch_dino_main\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input image: torch.Size([1, 3, 224, 224]) tensor(2.1458e-06) tensor(1.0000)\n",
      "Target image: torch.Size([1, 3, 224, 224]) tensor(3.9935e-06) tensor(1.0000)\n",
      "Perda perceptual: 24.463768005371094\n",
      "Tempo de execução: 0.25011420249938965 segundos\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor(24.4638)"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Exemplos de uso\n",
    "# Escolha o modelo desejado\n",
    "model = torch.hub.load('facebookresearch/dino:main', 'dinov2_vits14')\n",
    "\n",
    "# Instancie a função de perda perceptual\n",
    "perceptual_loss = PerceptualLoss(model=model,normalize_inputs=False)\n",
    "\n",
    "# Defina imagens de exemplo\n",
    "input_image = torch.rand(1, 3, 224, 224)  # Batch de 1 imagem (C, H, W)\n",
    "target_image = torch.rand(1, 3, 224, 224)\n",
    "\n",
    "print(\"Input image:\", input_image.shape, input_image.min(), input_image.max())\n",
    "print(\"Target image:\", target_image.shape, target_image.min(), target_image.max())\n",
    "# Compute a perda perceptual\n",
    "loss_value = perceptual_loss(input_image, target_image)\n",
    "print(\"Perda perceptual:\", loss_value.item())\n",
    "\n",
    "measure_execution_time(perceptual_loss, input_image, target_image)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DINO V2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /home/pdi_4/.cache/torch/hub/facebookresearch_dinov2_main\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input image: torch.Size([1, 3, 224, 224]) tensor(5.2452e-06) tensor(1.0000)\n",
      "Target image: torch.Size([1, 3, 224, 224]) tensor(2.3246e-06) tensor(1.0000)\n",
      "Perda perceptual: 18.904483795166016\n",
      "Tempo de execução: 0.11289119720458984 segundos\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor(18.9045)"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Exemplos de uso\n",
    "# Escolha o modelo desejado\n",
    "model = torch.hub.load('facebookresearch/dinov2', 'dinov2_vits14')\n",
    "\n",
    "\n",
    "# Instancie a função de perda perceptual\n",
    "perceptual_loss = PerceptualLoss(model=model,normalize_inputs=False)\n",
    "\n",
    "# Defina imagens de exemplo\n",
    "input_image = torch.rand(1, 3, 224, 224)  # Batch de 1 imagem (C, H, W)\n",
    "target_image = torch.rand(1, 3, 224, 224)\n",
    "\n",
    "print(\"Input image:\", input_image.shape, input_image.min(), input_image.max())\n",
    "print(\"Target image:\", target_image.shape, target_image.min(), target_image.max())\n",
    "# Compute a perda perceptual\n",
    "loss_value = perceptual_loss(input_image, target_image)\n",
    "print(\"Perda perceptual:\", loss_value.item())\n",
    "\n",
    "measure_execution_time(perceptual_loss, input_image, target_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /home/pdi_4/.cache/torch/hub/facebookresearch_dinov2_main\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input image: torch.Size([1, 3, 224, 224]) tensor(1.0490e-05) tensor(1.0000)\n",
      "Target image: torch.Size([1, 3, 224, 224]) tensor(2.6226e-06) tensor(1.0000)\n",
      "Perda perceptual: 13.746903419494629\n",
      "Tempo de execução: 0.34348106384277344 segundos\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor(13.7469)"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = torch.hub.load('facebookresearch/dinov2', 'dinov2_vitb14')\n",
    "\n",
    "# Instancie a função de perda perceptual\n",
    "perceptual_loss = PerceptualLoss(model=model,normalize_inputs=False)\n",
    "\n",
    "# Defina imagens de exemplo\n",
    "input_image = torch.rand(1, 3, 224, 224)  # Batch de 1 imagem (C, H, W)\n",
    "target_image = torch.rand(1, 3, 224, 224)\n",
    "\n",
    "print(\"Input image:\", input_image.shape, input_image.min(), input_image.max())\n",
    "print(\"Target image:\", target_image.shape, target_image.min(), target_image.max())\n",
    "# Compute a perda perceptual\n",
    "loss_value = perceptual_loss(input_image, target_image)\n",
    "print(\"Perda perceptual:\", loss_value.item())\n",
    "\n",
    "measure_execution_time(perceptual_loss, input_image, target_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /home/pdi_4/.cache/torch/hub/facebookresearch_dinov2_main\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input image: torch.Size([1, 3, 224, 224]) tensor(1.8418e-05) tensor(1.0000)\n",
      "Target image: torch.Size([1, 3, 224, 224]) tensor(5.4240e-06) tensor(1.0000)\n",
      "Perda perceptual: 23.665300369262695\n",
      "Tempo de execução: 1.2107529640197754 segundos\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor(23.6653)"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "model = torch.hub.load('facebookresearch/dinov2', 'dinov2_vitl14')\n",
    "\n",
    "# Instancie a função de perda perceptual\n",
    "perceptual_loss = PerceptualLoss(model=model,normalize_inputs=False)\n",
    "\n",
    "# Defina imagens de exemplo\n",
    "input_image = torch.rand(1, 3, 224, 224)  # Batch de 1 imagem (C, H, W)\n",
    "target_image = torch.rand(1, 3, 224, 224)\n",
    "\n",
    "print(\"Input image:\", input_image.shape, input_image.min(), input_image.max())\n",
    "print(\"Target image:\", target_image.shape, target_image.min(), target_image.max())\n",
    "# Compute a perda perceptual\n",
    "loss_value = perceptual_loss(input_image, target_image)\n",
    "print(\"Perda perceptual:\", loss_value.item())\n",
    "\n",
    "measure_execution_time(perceptual_loss, input_image, target_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /home/pdi_4/.cache/torch/hub/facebookresearch_dinov2_main\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input image: torch.Size([1, 3, 224, 224]) tensor(5.5432e-06) tensor(1.0000)\n",
      "Target image: torch.Size([1, 3, 224, 224]) tensor(2.1160e-05) tensor(1.0000)\n",
      "Perda perceptual: 33.627864837646484\n",
      "Tempo de execução: 3.510894536972046 segundos\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor(33.6279)"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "model = torch.hub.load('facebookresearch/dinov2', 'dinov2_vitg14')\n",
    "\n",
    "\n",
    "# Instancie a função de perda perceptual\n",
    "perceptual_loss = PerceptualLoss(model=model,normalize_inputs=False)\n",
    "\n",
    "# Defina imagens de exemplo\n",
    "input_image = torch.rand(1, 3, 224, 224)  # Batch de 1 imagem (C, H, W)\n",
    "target_image = torch.rand(1, 3, 224, 224)\n",
    "\n",
    "print(\"Input image:\", input_image.shape, input_image.min(), input_image.max())\n",
    "print(\"Target image:\", target_image.shape, target_image.min(), target_image.max())\n",
    "# Compute a perda perceptual\n",
    "loss_value = perceptual_loss(input_image, target_image)\n",
    "print(\"Perda perceptual:\", loss_value.item())\n",
    "\n",
    "measure_execution_time(perceptual_loss, input_image, target_image)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DINO V2 with registers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /home/pdi_4/.cache/torch/hub/facebookresearch_dinov2_main\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input image: torch.Size([1, 3, 224, 224]) tensor(6.9141e-06) tensor(1.0000)\n",
      "Target image: torch.Size([1, 3, 224, 224]) tensor(3.8147e-06) tensor(1.0000)\n",
      "Perda perceptual: 14.188370704650879\n",
      "Tempo de execução: 0.1164255142211914 segundos\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor(14.1884)"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = torch.hub.load('facebookresearch/dinov2', 'dinov2_vits14_reg')\n",
    "# Instancie a função de perda perceptual\n",
    "perceptual_loss = PerceptualLoss(model=model,normalize_inputs=False)\n",
    "\n",
    "# Defina imagens de exemplo\n",
    "input_image = torch.rand(1, 3, 224, 224)  # Batch de 1 imagem (C, H, W)\n",
    "target_image = torch.rand(1, 3, 224, 224)\n",
    "\n",
    "print(\"Input image:\", input_image.shape, input_image.min(), input_image.max())\n",
    "print(\"Target image:\", target_image.shape, target_image.min(), target_image.max())\n",
    "# Compute a perda perceptual\n",
    "loss_value = perceptual_loss(input_image, target_image)\n",
    "print(\"Perda perceptual:\", loss_value.item())\n",
    "\n",
    "measure_execution_time(perceptual_loss, input_image, target_image)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /home/pdi_4/.cache/torch/hub/facebookresearch_dinov2_main\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input image: torch.Size([1, 3, 224, 224]) tensor(7.5698e-06) tensor(1.0000)\n",
      "Target image: torch.Size([1, 3, 224, 224]) tensor(1.7881e-07) tensor(1.0000)\n",
      "Perda perceptual: 15.03640079498291\n",
      "Tempo de execução: 0.35260581970214844 segundos\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor(15.0364)"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = torch.hub.load('facebookresearch/dinov2', 'dinov2_vitb14_reg')\n",
    "# Instancie a função de perda perceptual\n",
    "perceptual_loss = PerceptualLoss(model=model,normalize_inputs=False)\n",
    "\n",
    "# Defina imagens de exemplo\n",
    "input_image = torch.rand(1, 3, 224, 224)  # Batch de 1 imagem (C, H, W)\n",
    "target_image = torch.rand(1, 3, 224, 224)\n",
    "\n",
    "print(\"Input image:\", input_image.shape, input_image.min(), input_image.max())\n",
    "print(\"Target image:\", target_image.shape, target_image.min(), target_image.max())\n",
    "# Compute a perda perceptual\n",
    "loss_value = perceptual_loss(input_image, target_image)\n",
    "print(\"Perda perceptual:\", loss_value.item())\n",
    "\n",
    "measure_execution_time(perceptual_loss, input_image, target_image)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /home/pdi_4/.cache/torch/hub/facebookresearch_dinov2_main\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input image: torch.Size([1, 3, 224, 224]) tensor(1.5199e-05) tensor(1.0000)\n",
      "Target image: torch.Size([1, 3, 224, 224]) tensor(1.0133e-06) tensor(1.0000)\n",
      "Perda perceptual: 33.44691467285156\n",
      "Tempo de execução: 1.1680426597595215 segundos\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor(33.4469)"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "model = torch.hub.load('facebookresearch/dinov2', 'dinov2_vitl14_reg')\n",
    "# Instancie a função de perda perceptual\n",
    "perceptual_loss = PerceptualLoss(model=model,normalize_inputs=False)\n",
    "\n",
    "# Defina imagens de exemplo\n",
    "input_image = torch.rand(1, 3, 224, 224)  # Batch de 1 imagem (C, H, W)\n",
    "target_image = torch.rand(1, 3, 224, 224)\n",
    "\n",
    "print(\"Input image:\", input_image.shape, input_image.min(), input_image.max())\n",
    "print(\"Target image:\", target_image.shape, target_image.min(), target_image.max())\n",
    "# Compute a perda perceptual\n",
    "loss_value = perceptual_loss(input_image, target_image)\n",
    "print(\"Perda perceptual:\", loss_value.item())\n",
    "\n",
    "measure_execution_time(perceptual_loss, input_image, target_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /home/pdi_4/.cache/torch/hub/facebookresearch_dinov2_main\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input image: torch.Size([1, 3, 224, 224]) tensor(3.0994e-06) tensor(1.0000)\n",
      "Target image: torch.Size([1, 3, 224, 224]) tensor(1.1325e-06) tensor(1.0000)\n",
      "Perda perceptual: 38.798404693603516\n",
      "Tempo de execução: 3.630220413208008 segundos\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor(38.7984)"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "model = torch.hub.load('facebookresearch/dinov2', 'dinov2_vitg14_reg')\n",
    "# Instancie a função de perda perceptual\n",
    "perceptual_loss = PerceptualLoss(model=model,normalize_inputs=False)\n",
    "\n",
    "# Defina imagens de exemplo\n",
    "input_image = torch.rand(1, 3, 224, 224)  # Batch de 1 imagem (C, H, W)\n",
    "target_image = torch.rand(1, 3, 224, 224)\n",
    "\n",
    "print(\"Input image:\", input_image.shape, input_image.min(), input_image.max())\n",
    "print(\"Target image:\", target_image.shape, target_image.min(), target_image.max())\n",
    "# Compute a perda perceptual\n",
    "loss_value = perceptual_loss(input_image, target_image)\n",
    "print(\"Perda perceptual:\", loss_value.item())\n",
    "\n",
    "measure_execution_time(perceptual_loss, input_image, target_image)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "provenance": []
  },
  "kernelspec": {
   "display_name": "CLEDiff",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
